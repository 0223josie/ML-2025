# Assignment Feedback: Week 3: Data Preparation

**Student:** 0223josie
**Raw Score:** 56/56 (100.0%)
**Course Points Earned:** 4

---

## Problem Breakdown

### __Exercise 1__ (10/10 = 100.0%)

**Part ex1-part1** (ex1-part1.code): 4/4 points

_Feedback:_ Good job. You loaded the data and clearly checked for nulls using both counts and percentages, which satisfies the requirement. Extra summary and outlier checks are fine but not required here.

**Part ex1-part2** (ex1-part2.code): 3/3 points

_Feedback:_ Good job. You produced a scatter plot matrix using pandas’ scatter_matrix and matplotlib. Selecting specific numeric columns is fine. Assuming these columns exist in your df, this meets the requirement. Consider plt.tight_layout() for cleaner spacing.

**Part ex1-part3** (ex1-part3.code): 3/3 points

_Feedback:_ Good job: you correctly plotted a bar chart of Product_Category counts (and others). The extra get_dummies step isn’t needed for plotting but doesn’t hurt. Consider adding axis labels for clarity, but your solution meets the requirement.

---

### __Exercise 2__ (10/10 = 100.0%)

**Part ex2-part1** (ex2-part1.code): 10/10 points

_Feedback:_ Good job. You imputed Age (median) and used KNN for Income and Last_Purchase_Amount, then verified nulls and saved the result. This meets the exercise goals and the rubric focus. Optional: if other columns had nulls, consider imputing them too.

---

### __Exercise 3__ (10/10 = 100.0%)

**Part ex3-part1** (ex3-part1.code): 10/10 points

_Feedback:_ Excellent. You load your imputed prior dataset, label-encode the target, one-hot encode categorical features, and standard-scale numeric features within pipelines, fitting on train to avoid leakage. Feature names handled correctly. Meets the exercise requirements fully.

---

### __Exercise 4__ (10/10 = 100.0%)

**Part ex4-part1** (ex4-part1.code): 10/10 points

_Feedback:_ Excellent. You correctly applied SMOTE on the training data, both within an imblearn Pipeline (avoiding leakage) and explicitly after preprocessing to verify class balance. Cross-validation with stratification is appropriate. Plots validate rebalancing.

---

### __Exercise 5__ (16/16 = 100.0%)

**Part ex5-part1** (ex5-part1.code): 6/6 points

_Feedback:_ Excellent. You trained LogisticRegression with CV, reported cross-validated accuracy and macro F1, and produced per-class F1 (via classification_report averaged across folds). Using an imblearn pipeline with SMOTE and your prior preprocess is appropriate. Full credit.

**Part ex5-part1** (ex5-part1.fill): 6/6 points

_Feedback:_ Well done. You built an IMB-learn pipeline with imputation, encoding, scaling, SMOTE, and LogisticRegression, and evaluated with cross_validate using f1_macro. Pipeline works on raw data. Minor: you created y_enc but didn’t use it. Otherwise correct.

**Part ex5-part2** (ex5-part2.code): 3/3 points

_Feedback:_ Good job: you built a DecisionTree pipeline with preprocess+SMOTE, compared it to Logistic Regression using CV accuracy and F1-macro, and printed results. This satisfies the task. For completeness, you could add a short print stating which model performed better based on the mean

**Part ex5-part3** (ex5-part3.answer): 1/1 points

_Feedback:_ Correct idea: SMOTE changes both X and y, which standard sklearn Pipelines don’t support (they transform X only and expect sample size unchanged). The imblearn Pipeline handles samplers that resample X and y together.

---

## Additional Information

This feedback was automatically generated by the autograder using LLM-based evaluation.

**Generated:** 2025-11-11 17:32:21 UTC

If you have questions about your grade, please reach out to the instructor.

---

*Powered by [Grade-Lite](https://github.com/your-repo/grade-lite) Autograder*