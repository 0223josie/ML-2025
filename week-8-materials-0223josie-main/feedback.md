# Assignment Feedback: Week 8: Regression Classification

**Student:** 0223josie
**Raw Score:** 26/27 (96.3%)
**Course Points Earned:** 4

---

## Problem Breakdown

### **Exercise 1** (10/10 = 100.0%)

**Part ex1-part1** (ex1-part1.fill): 4/4 points

_Feedback:_ Well done. You correctly imputed ‘embarked’ with SimpleImputer before encoding, performed OneHotEncoding and replaced original columns, and used KNNImputer on remaining features while preserving the target. End-to-end code runs. Minor: n_neighbors choice is fine; 3 would also be acceptable.

**Part ex1-part2** (ex1-part2.code): 3/3 points

_Feedback:_ Excellent use of ColumnTransformer with pipelines. You correctly impute categorical (most_frequent) before one-hot encoding (drop='first') and use KNNImputer(n_neighbors=5) for numeric columns, matching your prior workflow. Evaluation via CV and full pipeline are well done.

**Part ex1-part3** (ex1-part3.code): 3/3 points

_Feedback:_ Excellent. You correctly wrapped your ColumnTransformer in a Pipeline with the classifier and evaluated via cross_val_score, preventing data leakage. Your manual StratifiedKFold demo also fits the preprocessor on training folds only. Approach aligns with your prior work.

---

### **Exercise 2** (8/9 = 88.9%)

**Part ex2-part2** (ex2-part2.answer): 2/3 points

_Feedback:_ Good insight: logistic regression is linear and struggles on the non-linear moons, explaining the poor fit. However, you misstated the preprocessing: x1 was scaled by 100 and shifted by -42 (not 50), while x2 roughly spans −0.5 to 1.5. Mentioning the shift would improve accuracy.

**Part ex2-part3** (ex2-part3.code): 1/1 points

_Feedback:_ Good job: you scaled features via a pipeline and plotted predictions; this likely improves the linear model on the stretched x1. Consider also briefly stating what you observe and why (per prompt) about the decision boundary and accuracy after scaling.

**Part ex2-part4** (ex2-part4.answer): 2/2 points

_Feedback:_ Good insight: you correctly note that scaling won’t fix the non-linearity of the moons dataset, so logistic regression’s linear boundary still struggles and accuracy changes little. Minor note: scaling can still help optimization stability, but your conclusion is sound.

**Part ex2-part5** (ex2-part5.code): 1/1 points

_Feedback:_ Yes—your prior answer is correct: scaling alone didn’t help much because the moons problem is non-linear; the limitation is the linear decision boundary, not feature scale. Your exploration with polynomial features is fine but not required for this question.

**Part ex2-part6** (ex2-part6.answer): 2/2 points

_Feedback:_ Good insight: you note accuracy improves with degree and the trade-off between flexibility and overfitting, identifying degree≈3 as a balanced choice. Clear connection to decision boundary complexity. Minor phrasing issues, but understanding is solid.

---

### **Exercise 3** (8/8 = 100.0%)

**Part ex3-part2** (ex3-part2.code): 3/3 points

_Feedback:_ Good job: you built pipelines and correctly used evaluate_classifier to report mean CV performance. Printing mean accuracy aligns with your prior function. Minor redundancy (two KNN pipelines) is fine. Consider completing Step 3 visualization next.

**Part ex3-part3** (ex3-part3.code): 2/2 points

_Feedback:_ Well done. You evaluated two pipelines (KNN and LogisticRegression with PolynomialFeatures) using your evaluate_classifier with 5-fold CV, and correctly included scaling in both. Clear setup and results. Full credit.

**Part ex3-part4** (ex3-part4.answer): 2/2 points

_Feedback:_ Strong, accurate comparison of KNN vs polynomial Logistic Regression, noting instance-based vs model-based learning and practical trade-offs. Clear and correct. As a minor note, tying your comments to the decision-surface visualization/decision-function toggle would make it perfect.

**Part ex3-part5** (ex3-part5.code): 1/1 points

_Feedback:_ Good analysis. You compare KNN vs LR on moons, note KNN’s non-linear flexibility vs LR+polynomial features, and clearly state when to prefer each (noise level, dataset size, speed, interpretability). Solid reasoning tied to your experiments.

---

## Additional Information

This feedback was automatically generated by the autograder using LLM-based evaluation.

**Generated:** 2025-11-11 22:35:58 UTC

If you have questions about your grade, please reach out to the instructor.

---

*Powered by [Grade-Lite](https://github.com/your-repo/grade-lite) Autograder*