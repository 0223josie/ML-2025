{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Linear Regression and Gradient Descent\n",
    "\n",
    "## Linear Regression: The Basics\n",
    "\n",
    "Linear regression is a fundamental technique in machine learning used to model the relationship between one or more independent variables (features) and a dependent variable (target). At its core, linear regression attempts to fit a linear equation to observed data.\n",
    "\n",
    "### Simple Linear Regression\n",
    "\n",
    "Let's start with the simplest case: one feature and one target variable. The equation for simple linear regression is:\n",
    "\n",
    "$$ y = mx + b $$\n",
    "\n",
    "Where:\n",
    "- y is the predicted value\n",
    "- x is the feature value\n",
    "- m is the slope (also called the coefficient or weight)\n",
    "- b is the y-intercept (also called the bias)\n",
    "\n",
    "Visually, this looks like fitting a straight line to a set of data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate some sample data\n",
    "np.random.seed(0)\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = 2 * X + 1 + np.random.randn(100, 1) * 1\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, 2*X + 1, color='red')\n",
    "plt.title('Simple Linear Regression')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Multiple Linear Regression\n",
    "\n",
    "When we have multiple features, we extend this concept to multiple dimensions. The equation becomes:\n",
    "\n",
    "$$ y = w_1x_1 + w_2x_2 + ... + w_nx_n + b $$\n",
    "\n",
    "Where:\n",
    "- y is the predicted value\n",
    "- $x_1, x_2, ..., x_n$  are the feature values\n",
    "- $w_1, w_2, ..., w_n$ are the weights for each feature\n",
    "- $b$ is the bias term\n",
    "\n",
    "This is essentially the same as our simple linear regression, but now we're working in a higher-dimensional space. Instead of fitting a line, we're fitting a hyperplane.\n",
    "\n",
    "It's important to understand that the core concept remains the same whether we're working with one feature or many. We're still trying to find the best linear relationship between our features and the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Gradient Descent: Optimizing Our Model\n",
    "\n",
    "Now that we understand what linear regression is trying to do, how do we actually find the best values for our weights and bias? One of the most common approaches, used throughout machine learning, is called *gradient descent*.  \n",
    "\n",
    "## What is a Gradient?\n",
    "\n",
    "A gradient is a generalization of the concept of a derivative to functions of multiple variables. In the context of machine learning and optimization, the gradient represents the direction and rate of fastest increase of a function at a particular point.\n",
    "\n",
    "### From Slopes to Derivatives to Gradients\n",
    "\n",
    "1. **Slope**: In its simplest form, a slope is the steepness of a line. It's calculated as the change in $y$ divided by the change in $x$: $(y_2 - y_1) / (x_2 - x_1)$.\n",
    "\n",
    "2. **Derivative**: A derivative is the instantaneous rate of change of a function at any given point. It's the limit of the slope as the distance between two points approaches zero. For a function $f(x)$, the derivative is written as $f'(x)$ or $df/dx$.\n",
    "\n",
    "3. **Gradient**: The gradient is the multi-dimensional extension of the derivative. For a function of multiple variables,  $f(x_1, x_2, ..., x_n)$, the gradient is a vector of partial derivatives with respect to each variable:\n",
    "\n",
    "   $$\\nabla f = \\left[\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, ..., \\frac{\\partial f}{\\partial x_n}\\right]$$\n",
    "\n",
    "   Where $\\nabla$ is the gradient operator, and $\\frac{\\partial f}{\\partial x_1}$ is the partial derivative of $f$ with respect to $x_1$.\n",
    "\n",
    "In the context of linear regression, each component of the gradient represents how much the loss function would change if we slightly adjusted the corresponding weight or bias.\n",
    "\n",
    "## Determining Gradients\n",
    "\n",
    "Gradients are determined by calculating the partial derivatives of the function with respect to each of its variables. In practice, this often involves:\n",
    "\n",
    "1. **Analytical derivation**: Using calculus rules to derive the gradient formula.\n",
    "2. **Numerical approximation**: Using finite differences to estimate the gradient.\n",
    "3. **Automatic differentiation**: Leveraging computational graphs to automatically compute gradients (common in deep learning frameworks).\n",
    "\n",
    "For linear regression, we can analytically derive the gradients. Let's consider the Mean Squared Error (MSE) loss function:\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Where $n$ is the number of samples, $y_i$ is the true value, and $\\hat{y}_i$ is the predicted value.\n",
    "\n",
    "For a simple linear regression ($\\hat{y} = wx + b$), the gradients are:\n",
    "\n",
    "$$\\frac{\\partial \\text{MSE}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i) \\cdot x_i$$\n",
    "$$\\frac{\\partial \\text{MSE}}{\\partial b} = -\\frac{2}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)$$\n",
    "\n",
    "## What is a Loss Function?\n",
    "\n",
    "A loss function, also known as a cost function or objective function, measures how well our model's predictions match the actual data. It quantifies the \"error\" or \"loss\" associated with our current model parameters.\n",
    "\n",
    "Key points about loss functions:\n",
    "\n",
    "1. **Purpose**: They provide a single number representing the model's performance, which we aim to minimize.\n",
    "2. **Choice**: Different problems may require different loss functions. Common choices include:\n",
    "   - Mean Squared Error (MSE) for regression\n",
    "   - Cross-entropy for classification\n",
    "3. **Optimization**: Machine learning algorithms often work by minimizing the loss function through techniques like gradient descent.\n",
    "\n",
    "For linear regression, we commonly use the Mean Squared Error (MSE) as our loss function. MSE has several desirable properties:\n",
    "- It's always non-negative (squared terms)\n",
    "- It penalizes larger errors more heavily (quadratic)\n",
    "- It's differentiable, allowing us to compute gradients\n",
    "\n",
    "## Putting it All Together: Gradient Descent\n",
    "\n",
    "Gradient descent uses the gradients of the loss function to iteratively adjust the model parameters (weights and bias in linear regression) to minimize the loss. The process works as follows:\n",
    "\n",
    "1. Start with initial parameter values.\n",
    "2. Compute the gradient of the loss function with respect to each parameter.\n",
    "3. Update each parameter by subtracting a small step in the direction of its gradient:\n",
    "    $$w=w-\\text{learning\\_rate}\\cdot\\frac{\\partial\\text{MSE}}{\\partial w}$$\n",
    "    $$b=b-\\text{learning\\_rate}\\cdot\\frac{\\partial\\text{MSE}}{\\partial b}$$\n",
    "4. Repeat steps 2-3 until convergence or for a fixed number of iterations.\n",
    "\n",
    "The learning rate determines the size of the steps we take. Too large, and we might overshoot the minimum; too small, and convergence will be slow.\n",
    "\n",
    "Let's visualize this process for a simple 1D optimization problem:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    return x**2 + 5*np.sin(x)\n",
    "\n",
    "def df(x):\n",
    "    return 2*x + 5*np.cos(x)\n",
    "\n",
    "x = np.linspace(-10, 10, 200)\n",
    "y = f(x)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, y)\n",
    "plt.title('Optimization Landscape')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "\n",
    "# Gradient descent\n",
    "x_current = 8\n",
    "learning_rate = 0.1\n",
    "n_iterations = 50\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    plt.scatter(x_current, f(x_current), color='red')\n",
    "    x_current = x_current - learning_rate * df(x_current)\n",
    "\n",
    "plt.scatter(x_current, f(x_current), color='green', s=100, label='Final position')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### The Process of Gradient Descent\n",
    "\n",
    "1. Start with some initial values for the weights and bias.\n",
    "2. Calculate the predictions using these current parameter values.\n",
    "3. Compute the cost (error) between our predictions and the actual target values.\n",
    "4. Calculate the gradient of the cost with respect to each parameter.\n",
    "5. Update each parameter by subtracting a small portion of its gradient.\n",
    "6. Repeat steps 2-5 until the cost converges or we reach a maximum number of iterations.\n",
    "\n",
    "The \"small portion\" mentioned in step 5 is determined by the learning rate. A higher learning rate means larger steps, potentially reaching the minimum faster but risking overshooting. A lower learning rate means smaller, more careful steps, but may take longer to converge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Non-convex cost functions\n",
    "\n",
    "Note that for more complex problems, the optimization surface may be \"non-convex,\" which means that it might have several local optima, which means that gradient descent can get trapped.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    return x**4 - 4*x**2 + 5*np.sin(x)\n",
    "\n",
    "def df(x):\n",
    "    return 4*x**3 - 8*x + 5*np.cos(x)\n",
    "\n",
    "x = np.linspace(-3, 3, 300)\n",
    "y = f(x)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, y)\n",
    "plt.title('Non-Convex Optimization Landscape')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "\n",
    "# Gradient descent from multiple starting points\n",
    "learning_rate = 0.05\n",
    "n_iterations = 50\n",
    "starting_points = [-2.5, -1, 0, 1, 2.5]\n",
    "colors = ['red', 'green', 'blue', 'purple', 'orange']\n",
    "all_trajectories = []\n",
    "\n",
    "for start, color in zip(starting_points, colors):\n",
    "    x_current = start\n",
    "    trajectory = [x_current]\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        x_current = x_current - learning_rate * df(x_current)\n",
    "        trajectory.append(x_current)\n",
    "    \n",
    "    trajectory = np.array(trajectory)\n",
    "    all_trajectories.append(trajectory)\n",
    "    plt.scatter(trajectory, f(trajectory), color=color, s=30, alpha=0.5, label=f'Start: {start}')\n",
    "    plt.plot(trajectory, f(trajectory), color=color, alpha=0.3)\n",
    "    plt.scatter(trajectory[-1], f(trajectory[-1]), color=color, s=100, edgecolor='black')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print final positions\n",
    "idx = 0\n",
    "for start, color in zip(starting_points, colors):\n",
    "    final_x = all_trajectories[idx][-1]\n",
    "    idx+=1\n",
    "    print(f\"Starting at {start:.2f}, ended at {final_x:.2f} with value {f(final_x):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding example illustrates several different subtle problems.\n",
    "\n",
    "1. Multiple Local Minima:\n",
    "   The function we've chosen has multiple local minima. This is a common challenge in complex optimization problems, including many machine learning scenarios. In our example, depending on the starting point, gradient descent may converge to different local minima.\n",
    "\n",
    "2. Sensitivity to Initial Conditions:\n",
    "   The final result of the optimization process is highly dependent on the starting point. This highlights the importance of initialization in machine learning models, especially in deep learning where good initialization strategies can significantly impact model performance.\n",
    "\n",
    "3. Potential for Getting Stuck:\n",
    "   In some cases, the algorithm might get stuck in a local minimum that is not the global minimum. This is why more advanced optimization techniques often incorporate mechanisms to escape local minima.\n",
    "\n",
    "4. Plateaus and Saddle Points:\n",
    "   While not prominently featured in this specific function, many non-convex functions in higher dimensions have plateaus or saddle points where the gradient is close to zero. This can slow down or stall the optimization process.\n",
    "\n",
    "5. Learning Rate Sensitivity:\n",
    "   The choice of learning rate becomes more critical in non-convex scenarios. A large learning rate might cause the algorithm to overshoot and miss minima, while a small learning rate might result in slow convergence or getting stuck in poor local minima.\n",
    "\n",
    "6. Need for Multiple Runs:\n",
    "   In practice, when dealing with non-convex optimization problems, it's often necessary to run the optimization multiple times from different starting points to increase the chances of finding a good solution.\n",
    "\n",
    "7. Difficulty in Assessing Global Optimality:\n",
    "   Unlike convex optimization, where reaching a local minimum guarantees global optimality, in non-convex optimization, it's generally very difficult to determine if a found solution is globally optimal.\n",
    "\n",
    "Implications for Machine Learning:\n",
    "- In deep learning, the loss landscapes are typically highly non-convex. This is one reason why training deep neural networks can be challenging and why techniques like stochastic gradient descent, adaptive learning rates, and momentum have been developed.\n",
    "- Ensemble methods in machine learning can be seen as a way to mitigate the risks associated with non-convex optimization by combining multiple models, each potentially converging to different local optima.\n",
    "- Techniques like simulated annealing, genetic algorithms, and more advanced variants of gradient descent (e.g., Adam, RMSprop) are often employed to better navigate non-convex landscapes.\n",
    "\n",
    "This example and discussion highlight why optimization in machine learning is often more complex than simple gradient descent on convex functions, and why ongoing research in optimization algorithms remains crucial for advancing the field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Simple Linear Regression\n",
    "\n",
    "Linear regression for a simple, one dimensional problem is called Simple Linear Regression.  The heart of this is just the gradient descent. The following code illustrates how this can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate some sample data\n",
    "np.random.seed(0)\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = 2 * X + 1 + np.random.randn(100, 1) * 0.5\n",
    "\n",
    "# Initialize parameters\n",
    "w = 0\n",
    "b = 0\n",
    "learning_rate = 0.01\n",
    "n_iterations = 1000\n",
    "\n",
    "# Lists to store the parameter and MSE history\n",
    "w_history = [w]\n",
    "b_history = [b]\n",
    "mse_history = []\n",
    "\n",
    "# Gradient descent\n",
    "for i in range(n_iterations):\n",
    "    # Make predictions\n",
    "\n",
    "    y_pred = w * X + b\n",
    "    \n",
    "    # Compute MSE\n",
    "    mse = np.mean((y_pred - y) ** 2)\n",
    "    mse_history.append(mse)\n",
    "    \n",
    "    # Compute gradients\n",
    "    dw = (2 / len(X)) * np.sum((y_pred - y) * X)\n",
    "    db = (2 / len(X)) * np.sum(y_pred - y)\n",
    "    \n",
    "    # Update parameters\n",
    "    w -= learning_rate * dw\n",
    "    b -= learning_rate * db\n",
    "    \n",
    "    # Store parameter history\n",
    "    w_history.append(w)\n",
    "    b_history.append(b)\n",
    "    \n",
    "    # Print progress every 100 iterations\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iteration {i}: MSE = {mse:.4f}, w = {w:.4f}, b = {b:.4f}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot the data and the final regression line\n",
    "plt.subplot(121)\n",
    "plt.scatter(X, y, alpha=0.5)\n",
    "plt.plot(X, w*X + b, color='red')\n",
    "plt.title('Linear Regression with Gradient Descent')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Plot the MSE history\n",
    "plt.subplot(122)\n",
    "plt.plot(mse_history)\n",
    "plt.title('Mean Squared Error vs. Iterations')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('MSE')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final parameters: w = {w:.4f}, b = {b:.4f}\")\n",
    "\n",
    "# Plot parameter convergence\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(w_history)\n",
    "plt.title('Weight (w) Convergence')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('w')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(b_history)\n",
    "plt.title('Bias (b) Convergence')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('b')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Simple to Multiple Linear Regression: A Matrix Approach\n",
    "\n",
    "## Simple Linear Regression\n",
    "\n",
    "In simple linear regression, we have one independent variable $x$ and one dependent variable $y$. The model is represented as:\n",
    "\n",
    "$$y = wx + b$$\n",
    "\n",
    "Where $w$ is the weight (or slope) and $b$ is the bias (or intercept).\n",
    "\n",
    "For $n$ data points, we can write this as a system of equations:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y_1 &= wx_1 + b \\\\\n",
    "y_2 &= wx_2 + b \\\\\n",
    "&\\vdots \\\\\n",
    "y_n &= wx_n + b\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## Multiple Linear Regression\n",
    "\n",
    "In multiple linear regression, we have multiple independent variables $x_1, x_2, ..., x_m$ and one dependent variable $y$. The model is represented as:\n",
    "\n",
    "$$y = w_1x_1 + w_2x_2 + ... + w_mx_m + b$$\n",
    "\n",
    "For $n$ data points and $m$ features, we can write this as a system of equations:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y_1 &= w_1x_{11} + w_2x_{12} + ... + w_mx_{1m} + b \\\\\n",
    "y_2 &= w_1x_{21} + w_2x_{22} + ... + w_mx_{2m} + b \\\\\n",
    "&\\vdots \\\\\n",
    "y_n &= w_1x_{n1} + w_2x_{n2} + ... + w_mx_{nm} + b\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## Matrix Representation\n",
    "\n",
    "We can represent this system of equations using matrices:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{X}\\mathbf{w} + \\mathbf{b}$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{y}$ is an $n \\times 1$ vector of dependent variables\n",
    "- $\\mathbf{X}$ is an $n \\times (m+1)$ matrix of independent variables (including a column of 1s for the bias term)\n",
    "- $\\mathbf{w}$ is an $(m+1) \\times 1$ vector of weights (including the bias as the last element)\n",
    "\n",
    "In expanded form:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1m} & 1 \\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2m} & 1 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "x_{n1} & x_{n2} & \\cdots & x_{nm} & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_m \\\\\n",
    "b\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Understanding np.dot\n",
    "\n",
    "The NumPy function `np.dot` performs matrix multiplication. When we use `np.dot(X, w)`, we're essentially computing the sum of element-wise multiplications for each row of $\\mathbf{X}$ with $\\mathbf{w}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y_1 &= x_{11}w_1 + x_{12}w_2 + ... + x_{1m}w_m + 1 \\cdot b \\\\\n",
    "y_2 &= x_{21}w_1 + x_{22}w_2 + ... + x_{2m}w_m + 1 \\cdot b \\\\\n",
    "&\\vdots \\\\\n",
    "y_n &= x_{n1}w_1 + x_{n2}w_2 + ... + x_{nm}w_m + 1 \\cdot b\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This operation efficiently computes the predictions for all data points simultaneously.\n",
    "\n",
    "## Gradient Computation\n",
    "\n",
    "The gradient of the Mean Squared Error (MSE) loss function with respect to the weights can also be computed using matrix operations:\n",
    "\n",
    "$$\\nabla_w \\text{MSE} = \\frac{2}{n} \\mathbf{X}^T (\\mathbf{X}\\mathbf{w} - \\mathbf{y})$$\n",
    "\n",
    "Where $\\mathbf{X}^T$ is the transpose of $\\mathbf{X}$.\n",
    "\n",
    "This matrix operation efficiently computes the gradients for all weights simultaneously, allowing for vectorized implementations of gradient descent.\n",
    "\n",
    "## Practical Implementation\n",
    "\n",
    "In practice, we can implement this in NumPy as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(0)\n",
    "n_samples = 100\n",
    "n_features = 2\n",
    "\n",
    "# Create feature matrix X with two features\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Create target vector y\n",
    "true_weights = [2, -3.5]\n",
    "true_bias = 5\n",
    "y = np.dot(X, true_weights) + true_bias + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "# Add a column of 1s to X for the bias term\n",
    "# np.column_stack concatenates arrays along the second axis (columns)\n",
    "# np.ones(X.shape[0]) creates an array of 1s with the same number of rows as X\n",
    "X_with_bias = np.column_stack([X, np.ones(X.shape[0])])\n",
    "\n",
    "# Initialize weights (including bias) with zeros\n",
    "# The shape is X_with_bias.shape[1] to match the number of features plus the bias term\n",
    "w = np.zeros(X_with_bias.shape[1])\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_iterations = 1000\n",
    "\n",
    "# Lists to store the loss history\n",
    "loss_history = []\n",
    "\n",
    "# Gradient descent\n",
    "for i in range(n_iterations):\n",
    "    # Compute predictions using matrix multiplication\n",
    "    y_pred = np.dot(X_with_bias, w)\n",
    "    \n",
    "    # Compute the loss (Mean Squared Error)\n",
    "    loss = np.mean((y_pred - y) ** 2)\n",
    "    loss_history.append(loss)\n",
    "    \n",
    "    # Compute gradients using matrix operations\n",
    "    # The transpose of X_with_bias (X_with_bias.T) is used to match dimensions for matrix multiplication\n",
    "    gradients = (2 / n_samples) * np.dot(X_with_bias.T, (y_pred - y))\n",
    "    \n",
    "    # Update weights using gradient descent\n",
    "    w -= learning_rate * gradients\n",
    "\n",
    "    # Print progress every 100 iterations\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iteration {i}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Print the final weights and bias\n",
    "print(\"Final weights:\", w[:-1])\n",
    "print(\"Final bias:\", w[-1])\n",
    "\n",
    "# Visualize the loss history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_history)\n",
    "plt.title('Loss vs. Iterations')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()\n",
    "\n",
    "# For 2D data, we can visualize the regression plane\n",
    "if n_features == 2:\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot the original data points\n",
    "    ax.scatter(X[:, 0], X[:, 1], y, c='b', marker='o', alpha=0.5)\n",
    "    \n",
    "    # Create a meshgrid for the regression plane\n",
    "    x0_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 50)\n",
    "    x1_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 50)\n",
    "    X0, X1 = np.meshgrid(x0_range, x1_range)\n",
    "    \n",
    "    # Compute the predicted values for the meshgrid\n",
    "    Z = w[0] * X0 + w[1] * X1 + w[2]\n",
    "    \n",
    "    # Plot the regression plane\n",
    "    ax.plot_surface(X0, X1, Z, alpha=0.3, cmap='viridis')\n",
    "    \n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_zlabel('Target')\n",
    "    ax.set_title('Linear Regression in 3D')\n",
    "\n",
    "    elev = 10   # elevation angle in degrees\n",
    "    azim = 45   # azimuth angle in degrees\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The bridge to practice: Linear Regression in scikit‑learn\n",
    "\n",
    "In real projects, we rarely hand‑roll optimizers. scikit‑learn gives you a consistent, simple API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "print(\"Coefficient (w):\", linreg.coef_)\n",
    "print(\"Intercept (b):\", linreg.intercept_)\n",
    "y_pred = linreg.predict(X_test)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Takeaway: same concept, but the library handles optimization details, numerical stability, and performance for you.\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluating Regressors (R², RMSE, MAE)\n",
    "\n",
    "When you evaluate **regression**, you’ll usually report **R²**, **RMSE**, and **MAE**:\n",
    "\n",
    "* **R²**: proportion of variance explained (unitless, higher is better; can be negative on poor generalization).\n",
    "* **RMSE**: root mean squared error (same units as target; penalizes large errors more).\n",
    "* **MAE**: mean absolute error (same units; more robust to outliers).\n",
    "\n",
    "Unlike classification scores such as **F1**, RMSE/MAE **depend on the units** of $y$, so interpretation needs context (e.g., “RMSE ≈ \\$12k” for house prices).&#x20;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "When evaluating regression models, three commonly used metrics are R² (R-squared), RMSE (Root Mean Square Error), and MAE (Mean Absolute Error). Let's explore these metrics in detail and see how to implement them in Python.\n",
    "\n",
    "#### R² (R-squared) Score\n",
    "\n",
    "R², also known as the coefficient of determination, is a statistical measure that represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It provides an indication of the goodness of fit of a model.\n",
    "\n",
    "- R² ranges from 0 to 1.\n",
    "- An R² of 0 indicates that the model explains none of the variability of the data.\n",
    "- An R² of 1 indicates that the model explains all the variability of the data.\n",
    "\n",
    "##### Formula\n",
    "\n",
    "$$ R^2 = 1 - \\frac{SSR}{SST} $$\n",
    "Where:\n",
    "\n",
    "SSR is the sum of squared residuals\n",
    "SST is the total sum of squares\n",
    "\n",
    "More specifically:\n",
    "$$ R^2 = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}i)^2}{\\sum{i=1}^n (y_i - \\bar{y})^2} $$\n",
    "Where:\n",
    "\n",
    "$y_i$ are the observed values\n",
    "$\\hat{y}_i$ are the predicted values\n",
    "$\\bar{y}$ is the mean of the observed data\n",
    "\n",
    "\n",
    "##### Interpretation\n",
    "\n",
    "- An R² of 0.7 means that 70% of the variance in the target variable can be explained by the model.\n",
    "- Higher R² values indicate a better fit, but be cautious of overfitting when R² is very close to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### RMSE (Root Mean Square Error)\n",
    "\n",
    "RMSE (which we've discussed before) is a frequently used measure of the differences between values predicted by a model and the values actually observed. It represents the standard deviation of the residuals (prediction errors).\n",
    "\n",
    "- RMSE is always non-negative, and a value of 0 indicates a perfect fit to the data.\n",
    "- It has the same units as the dependent variable.\n",
    "- Lower values of RMSE indicate better fit.\n",
    "\n",
    "#### Formula\n",
    "\n",
    "The formula for RMSE is:\n",
    "\n",
    "$$ RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2} $$\n",
    "Where:\n",
    "\n",
    "$n$ is the number of observations\n",
    "$y_i$ are the observed values\n",
    "$\\hat{y}_i$ are the predicted values\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- RMSE can be interpreted as the average deviation of the predictions from the observed values.\n",
    "- It gives more weight to large errors due to the squaring operation.\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "Mean Absolute Error (MAE) is another common metric used to evaluate regression models. It measures the average magnitude of the errors in a set of predictions, without considering their direction. \n",
    "\n",
    "#### Formula\n",
    "\n",
    "The formula for MAE is:\n",
    "\n",
    "$$ MAE = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i| $$\n",
    "\n",
    "Where:\n",
    "- $n$ is the number of observations\n",
    "- $y_i$ are the observed values\n",
    "- $\\hat{y}_i$ are the predicted values\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- MAE is always non-negative, and a value of 0 indicates a perfect fit to the data.\n",
    "- It has the same units as the dependent variable.\n",
    "- Lower values of MAE indicate better fit.\n",
    "- MAE represents the average absolute difference between predicted and actual values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Comparing MAE and RMSE\n",
    "\n",
    "Both MAE and RMSE are commonly used metrics for regression problems, but they have some key differences:\n",
    "\n",
    "1. **Interpretation**: \n",
    "   - MAE is easier to interpret as it's in the same units as the target variable and represents the average absolute error.\n",
    "   - RMSE is in the same units as the target variable, but it represents the standard deviation of the residuals.\n",
    "\n",
    "2. **Sensitivity to outliers**:\n",
    "   - MAE is less sensitive to outliers because it doesn't square the errors.\n",
    "   - RMSE gives higher weight to large errors due to the squaring operation, making it more sensitive to outliers.\n",
    "\n",
    "3. **Mathematical properties**:\n",
    "   - MAE is based on the L1 norm (sum of absolute values).\n",
    "   - RMSE is based on the L2 norm (sum of squared values).\n",
    "\n",
    "4. **Formula comparison**:\n",
    "   MAE: $\\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|$\n",
    "   RMSE: $\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}$\n",
    "\n",
    "5. **Use cases**:\n",
    "   - MAE is preferred when you want to treat all errors equally.\n",
    "   - RMSE is preferred when large errors are particularly undesirable, as it penalizes them more heavily.\n",
    "\n",
    "\n",
    "#### Choosing Between MAE and RMSE\n",
    "\n",
    "- Use MAE when you want to treat all errors equally and when outliers are not particularly problematic for your application.\n",
    "- Use RMSE when large errors are especially undesirable, or when you want to maintain mathematical properties like differentiability (RMSE is differentiable everywhere, while MAE is not differentiable at 0).\n",
    "- Often, it's beneficial to report both metrics to provide a more comprehensive view of your model's performance.\n",
    "\n",
    "Remember, the choice between MAE and RMSE (or using both) can depend on your specific problem, the nature of your data, and the requirements of your stakeholders.\n",
    "\n",
    "\n",
    "## Using sklearn\n",
    "\n",
    "Scikit-learn provides easy to use implementations most common metrics in the \"metrics\" package of the library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "y_true = np.array([3, -0.5, 2, 7])\n",
    "y_pred = np.array([2.5, 0.0, 2, 8])\n",
    "\n",
    "# R² score\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "# MAE\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(f\"MAE: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"R²:  {r2:.3f}\")\n",
    "print(f\"RMSE:{rmse:.3f}\")\n",
    "print(f\"MAE: {mae:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross‑validation for more reliable estimates\n",
    "\n",
    "Single train/test splits can be noisy. Use **cross‑validation** to average performance across folds. In scikit‑learn, “negative” MSE is reported (so that higher is better); take the square root of the **negated** values to get RMSE.&#x20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "scoring = {\n",
    "    'R2': 'r2',\n",
    "    'neg_MSE': 'neg_mean_squared_error',\n",
    "    'MAE': 'neg_mean_absolute_error'  # note: negative form in CV API\n",
    "}\n",
    "cv_results = cross_validate(linreg, X, y, cv=5, scoring=scoring, return_train_score=False)\n",
    "\n",
    "# Convert negatives and compute RMSE\n",
    "rmse_scores = np.sqrt(-cv_results['test_neg_MSE'])\n",
    "mae_scores = -cv_results['test_MAE']\n",
    "\n",
    "print(f\"R² (mean ± 2sd): {cv_results['test_R2'].mean():.3f} ± {2*cv_results['test_R2'].std():.3f}\")\n",
    "print(f\"RMSE (mean ± 2sd): {rmse_scores.mean():.3f} ± {2*rmse_scores.std():.3f}\")\n",
    "print(f\"MAE (mean ± 2sd): {mae_scores.mean():.3f} ± {2*mae_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Training vs. Generalization**: Always focus on scores computed on validation/test folds rather than training scores—good training error with poor test error indicates overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Regression ↔ Classification: same patterns, different goals\n",
    "\n",
    "A nice scikit‑learn feature: **estimators come in regressor/classifier pairs** with nearly identical APIs.\n",
    "\n",
    "* **Tree/ensemble example**\n",
    "\n",
    "  * `RandomForestRegressor` predicts a continuous value; evaluate with RMSE/MAE/R².\n",
    "  * `RandomForestClassifier` predicts a class; evaluate with accuracy, precision, recall, F1, ROC‑AUC, etc.\n",
    "\n",
    "* **Linear vs. Logistic**\n",
    "\n",
    "  * `LinearRegression` models a numeric target.\n",
    "  * `LogisticRegression` models class probabilities (classification).\n",
    "\n",
    "Practically, if you learn one, you’re most of the way to the other—the **fit/predict** loop, **train\\_test\\_split**, and **cross‑validation** all look the same. The **main caveat** is evaluation: **classification metrics are unit‑less** (e.g., F1), while **regression error is in the units of the target**, which can make interpretation trickier.&#x20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Regression toy example\n",
    "Xr, yr = make_regression(n_samples=500, n_features=8, noise=10.0, random_state=42)\n",
    "Xr_tr, Xr_te, yr_tr, yr_te = train_test_split(Xr, yr, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_reg = RandomForestRegressor(random_state=42)\n",
    "rf_reg.fit(Xr_tr, yr_tr)\n",
    "yr_pred = rf_reg.predict(Xr_te)\n",
    "print(\"Regressor RMSE:\", np.sqrt(mean_squared_error(yr_te, yr_pred)).round(3))\n",
    "\n",
    "# Classification toy example\n",
    "Xc, yc = make_classification(n_samples=500, n_features=8, weights=[0.7, 0.3], random_state=42)\n",
    "Xc_tr, Xc_te, yc_tr, yc_te = train_test_split(Xc, yc, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(Xc_tr, yc_tr)\n",
    "yc_pred = rf_clf.predict(Xc_te)\n",
    "\n",
    "print(\"Classifier F1:\", round(f1_score(yc_te, yc_pred),3), \"| Accuracy:\", round(accuracy_score(yc_te, yc_pred),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: When you move from regression to classification, swap the **estimator** and **metrics**; your data handling and code structure can stay almost the same.\n",
    "\n",
    "---\n",
    "\n",
    "## Pipelines + data leakage \n",
    "\n",
    "Recalling our prior lecture on data leakage, it's important in supervised learning to do any scaling/encoding features **inside a Pipeline** so they’re learned on the **training fold only** and then applied to validation/test data—avoiding **data leakage**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: quick preview of a regression pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "reg_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "scores = cross_validate(reg_pipeline, X, y, cv=5, scoring={'R2':'r2','neg_MSE':'neg_mean_squared_error'})\n",
    "np.sqrt(-scores['test_neg_MSE']).mean(), scores['test_R2'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
