1. Can you give me three reasons to use regularization?
2. What is the difference between L1 and L2 regularization?
3. How do you know what an appropriate choice of lambda for l1/L2 regularization?
4. What is dropout?  How do you use it in a keras network?
5. How should one determine the dropout rate?
6. Why can the training accuracy be lower than validation accuracy with dropout? 
7. With what type of learning optimizer should you *not* use simple Dropout?
7. What is monte carlo dropout?
8. What kind of ensemble technique is MC Dropout similar to?
9. How can we implement MC Dropout in Keras?
10. What is MaxNorm regularization? 
11. How do you implement Max-Norm regularization in Keras? 
12. What do you think about combining Max-Norm regularization with SELU?


