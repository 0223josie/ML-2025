{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d80c79c",
   "metadata": {},
   "source": [
    "# Applied Machine Learning: Pipelines in scikit-learn\n",
    "\n",
    "## 1. Why Use Pipelines?\n",
    "\n",
    "In real-world machine learning, we rarely train a model on raw data directly.  \n",
    "We often need to:\n",
    "- Handle missing values\n",
    "- Transform numeric variables (scaling, log transform, etc.)\n",
    "- Encode categorical variables\n",
    "- Balance imbalanced classes\n",
    "- Train a model\n",
    "\n",
    "If we do these steps **separately**, we run into two main problems:\n",
    "\n",
    "### 1.1 Data Leakage\n",
    "\n",
    "**Data leakage** happens when information from outside the training dataset is accidentally used to create the model.  \n",
    "This can lead to overly optimistic results during training but poor performance on new data.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a219b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Example data\n",
    "df = pd.DataFrame({\n",
    "    \"feature\": [1, 2, 3, 4, 5, 6],\n",
    "    \"target\": [0, 0, 0, 1, 1, 1]\n",
    "})\n",
    "\n",
    "X = df[[\"feature\"]]\n",
    "y = df[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# BAD: Fitting the scaler on ALL data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)   # <-- Leakage: test data influences scaling\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "````\n",
    "\n",
    "Here, the scaling parameters (mean and std) are computed using **both training and test data**, which \"leaks\" information from the test set into the training process.\n",
    "\n",
    "Pipelines solve this by **fitting transformations only on the training set**, and then applying the same transformation to the test set.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Maintainability and Reuse\n",
    "\n",
    "If you have a single block of code to preprocess, train, and predict, it's easy to:\n",
    "\n",
    "* Forget steps when making predictions on new data\n",
    "* Apply steps in the wrong order\n",
    "* Duplicate code in multiple places\n",
    "\n",
    "Pipelines make the sequence of steps explicit and reusable.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The Basics of Pipelines\n",
    "\n",
    "A pipeline is a sequence of steps, each of which is either:\n",
    "\n",
    "* A **transformer** (preprocessing, imputation, scaling, encoding, etc.)\n",
    "* A **final estimator** (classifier, regressor, etc.)\n",
    "\n",
    "Example: scaling → logistic regression\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "predictions = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f51eada",
   "metadata": {},
   "source": [
    "**Key points:**\n",
    "\n",
    "* The steps are applied in order.\n",
    "* All steps except the last must be transformers (with `.fit` and `.transform` methods).\n",
    "* The last step is the model.\n",
    "\n",
    "---\n",
    "\n",
    "Great — here’s the **updated markdown** with a simple diagram added after Section 3.\n",
    "I’ve kept it text-based using Mermaid so that in Jupyter they can either leave it as-is or render it with `jupyterlab-mermaid` or just treat it as a conceptual sketch.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Why Pipelines Prevent Leakage\n",
    "\n",
    "When you call `.fit(X_train, y_train)`:\n",
    "1. Each transformer step is **fit only on `X_train`**.\n",
    "2. The transformed data is passed to the next step.\n",
    "3. The final estimator is fit on the fully transformed training data.\n",
    "\n",
    "When you call `.predict(X_test)`:\n",
    "1. Transformers use the parameters learned from the training set.\n",
    "2. The transformed test data is passed to the final estimator.\n",
    "\n",
    "---\n",
    "\n",
    "### Visualizing the Flow\n",
    "_Note: You'll need to look at this on Github for it to render properly_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d94f3d",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart TB\n",
    "    A[Raw Data] --> B[Step 1: Transformer 1 <br/> #0040; e.g., Imputer #0041;]\n",
    "    B --> C[Step 2: Transformer 2 <br/> #0040; e.g., Scaler or Encoder #0041;]\n",
    "    C --> D[Final Estimator <br/> #0040; e.g., Random Forest #0041;]\n",
    "    D --> E[Predictions]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ca12fb",
   "metadata": {},
   "source": [
    "**How to read this:**\n",
    "\n",
    "* Each transformation step takes the output from the previous step.\n",
    "* Transformers are *fit* only on the training set.\n",
    "* The final estimator produces predictions based on the transformed data.\n",
    "* At prediction time, the same fitted transformers are applied in the same order to new data.\n",
    "\n",
    "---\n",
    "\n",
    "**Why this matters for leakage:**\n",
    "\n",
    "* If we fit a scaler or encoder *before* splitting the data, we \"peek\" at the test set.\n",
    "* Pipelines make sure that each transformation is **fit only on training folds** during cross-validation and only on the training set during final training.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Preprocessing Multiple Column Types\n",
    "\n",
    "Real datasets have:\n",
    "\n",
    "* Numeric columns (need scaling, log transform, imputation)\n",
    "* Categorical columns (need one-hot encoding, imputation)\n",
    "\n",
    "We can process them separately and then combine them with **`ColumnTransformer`**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1 Introduction to `ColumnTransformer`\n",
    "\n",
    "`ColumnTransformer` applies different preprocessing to different columns in one step.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91561e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_features = ['feature']\n",
    "categorical_features = ['color']\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3419f98",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Visualizing ColumnTransformer Flow\n",
    "_Note: You'll need to look at this on Github for it to render properly_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bc1fe4",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart TB\n",
    "    A[Raw Data] --> B[Numeric Columns]\n",
    "    A --> C[Categorical Columns]\n",
    "\n",
    "    B --> B1[Numeric Imputer]\n",
    "    B1 --> B2[Scaler]\n",
    "\n",
    "    C --> C1[Categorical Imputer]\n",
    "    C1 --> C2[One-Hot Encoder]\n",
    "\n",
    "    B2 --> D[Combined Features]\n",
    "    C2 --> D\n",
    "\n",
    "    D --> E[Model]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160aaed5",
   "metadata": {},
   "source": [
    "**How to read this:**\n",
    "\n",
    "* The data is split into **numeric** and **categorical** subsets.\n",
    "* Each subset goes through its own sequence of transformations.\n",
    "* The processed numeric and categorical features are recombined into a single dataset.\n",
    "* The combined dataset is fed into the model.\n",
    "\n",
    "---\n",
    "\n",
    "This mental model helps explain:\n",
    "\n",
    "* Why preprocessing can happen in parallel for different column types.\n",
    "* How missing data and scaling/encoding are handled independently for each type.\n",
    "* That the order **within each branch** still matters, but branches don’t affect each other.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Combining `ColumnTransformer` with a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8391dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9447f118",
   "metadata": {},
   "source": [
    "This structure:\n",
    "\n",
    "* Cleans and transforms each column type\n",
    "* Passes processed data to the model\n",
    "* Keeps everything together in one object\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Imputation and Pipelines\n",
    "\n",
    "Missing values are common.\n",
    "We can impute (fill in) missing data **inside** the pipeline so it’s done correctly for both training and new data.\n",
    "\n",
    "Types of imputation:\n",
    "\n",
    "* Mean/median for numeric\n",
    "* Most frequent for categorical\n",
    "* Constant value\n",
    "\n",
    "Pipelines ensure imputation is **fit only on training data**.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Handling Imbalanced Data with `imblearn` Pipelines\n",
    "\n",
    "When classes are imbalanced (e.g., 95% class A, 5% class B), we may want to use **oversampling** or **undersampling**.\n",
    "\n",
    "The `imblearn.pipeline.Pipeline` works like sklearn’s Pipeline but supports samplers like SMOTE.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3434b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "imb_pipe = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('smote', SMOTE()),\n",
    "    ('model', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "imb_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ce9042",
   "metadata": {},
   "source": [
    "**Important:** Sampling must be inside the pipeline to avoid leakage.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Grid Search with Pipelines\n",
    "\n",
    "We can tune hyperparameters without worrying about leakage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e081312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'model__n_estimators': [50, 100],\n",
    "    'model__max_depth': [None, 5]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aaa5bf",
   "metadata": {},
   "source": [
    "The CV process will:\n",
    "\n",
    "* Fit the preprocessors on the training folds only\n",
    "* Apply them to validation folds without refitting\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Summary\n",
    "\n",
    "* **Pipelines** combine preprocessing and modeling steps into one object.\n",
    "* They **prevent data leakage** by fitting transformations only on training data.\n",
    "* **ColumnTransformer** lets you preprocess numeric and categorical features differently.\n",
    "* **imblearn Pipelines** allow resampling inside the pipeline.\n",
    "* Pipelines make your code **cleaner, safer, and reusable**.\n",
    "\n",
    "---\n",
    "\n",
    "### Practice Ideas\n",
    "\n",
    "1. Build a pipeline for a dataset with both numeric and categorical columns.\n",
    "2. Add missing values and handle them with imputation in the pipeline.\n",
    "3. Try an imbalanced dataset and use SMOTE in the pipeline.\n",
    "4. Tune hyperparameters using GridSearchCV on the pipeline.\n",
    "\n",
    "Perfect — I’ll extend the markdown with a **small, realistic CSV-based example** that walks students from messy raw data all the way to predictions using a pipeline.\n",
    "\n",
    "I’ll keep it simple but with enough “realism” to hit:\n",
    "\n",
    "* Missing values in numeric & categorical features\n",
    "* Numeric scaling\n",
    "* Categorical one-hot encoding\n",
    "* Model training\n",
    "* GridSearchCV for tuning\n",
    "\n",
    "---\n",
    "\n",
    "## 10. End-to-End Example: From Messy CSV to Predictions\n",
    "\n",
    "Let's put it all together with a small, realistic dataset.\n",
    "\n",
    "### Step 1: Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68af7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example messy dataset\n",
    "data = pd.DataFrame({\n",
    "    \"age\": [25, 30, None, 45, 22, None, 37, 29],\n",
    "    \"income\": [50000, 60000, 55000, None, 42000, 52000, None, 58000],\n",
    "    \"job_type\": [\"office\", \"manual\", \"manual\", None, \"office\", \"office\", \"manual\", \"manual\"],\n",
    "    \"owns_car\": [\"yes\", \"no\", \"yes\", \"yes\", \"no\", \"no\", None, \"yes\"],\n",
    "    \"bought_insurance\": [0, 1, 0, 1, 0, 0, 1, 1]\n",
    "})\n",
    "\n",
    "X = data.drop(\"bought_insurance\", axis=1)\n",
    "y = data[\"bought_insurance\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9dd8d9",
   "metadata": {},
   "source": [
    "We have:\n",
    "\n",
    "* Missing values in both numeric and categorical features\n",
    "* A mix of numeric and categorical columns\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Define Preprocessing\n",
    "\n",
    "We’ll use:\n",
    "\n",
    "* **Mean imputation + scaling** for numeric features\n",
    "* **Most frequent imputation + one-hot encoding** for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1c47af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_features = [\"age\", \"income\"]\n",
    "categorical_features = [\"job_type\", \"owns_car\"]\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973edaff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 3: Create the Full Pipeline\n",
    "\n",
    "We'll use a **Random Forest** as the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f8089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', RandomForestClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27773ae1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 4: Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924396d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b23934",
   "metadata": {},
   "source": [
    "At this point:\n",
    "\n",
    "* Missing values were handled **inside** the pipeline\n",
    "* Scaling and encoding were done correctly without leakage\n",
    "* The model received clean, numeric input automatically\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b531e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'model__n_estimators': [50, 100],\n",
    "    'model__max_depth': [None, 5, 10]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best score:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e159ab",
   "metadata": {},
   "source": [
    "**Note:** The parameters are referenced using the syntax `stepname__parameter`.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6: Predict on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4529d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_customers = pd.DataFrame({\n",
    "    \"age\": [40, None],\n",
    "    \"income\": [62000, 48000],\n",
    "    \"job_type\": [\"manual\", None],\n",
    "    \"owns_car\": [\"no\", \"yes\"]\n",
    "})\n",
    "\n",
    "preds = grid.predict(new_customers)\n",
    "print(\"Predictions for new customers:\", preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59733bcb",
   "metadata": {},
   "source": [
    "Even though our new data has missing values, the pipeline handles it automatically — no extra preprocessing code needed.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways from the Example\n",
    "\n",
    "* All preprocessing steps are **fit on training data only** — no leakage.\n",
    "* The pipeline can be applied directly to new, messy data.\n",
    "* Different feature types can have **different preprocessing** in the same pipeline.\n",
    "* Hyperparameter tuning with GridSearchCV works naturally with pipelines."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
