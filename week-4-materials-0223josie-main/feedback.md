# Assignment Feedback: Week 4: Dimensionality Reduction

**Student:** 0223josie
**Raw Score:** 47/50 (94.0%)
**Course Points Earned:** 4

---

## Problem Breakdown

### Exercise 2 (10/10 = 100.0%)

**Part ex1-part1** (ex1-part1.code): 4/4 points

_Feedback:_ Great job—this correctly applies t-SNE to MNIST and visualizes labels. Sensible subsampling and scaling, appropriate TSNE params, and clear plotting with colorbar. This meets the exercise goal. You could also try PCA init or tuning perplexity for variation.

**Part ex1-part2** (ex1-part2.code): 3/3 points

_Feedback:_ Good job: you computed a t-SNE embedding of your prior X_sub/y_sub, split into train/test, trained KNN, and reported accuracy. This addresses the question. For rigor, consider fitting t-SNE on the train set only to avoid leakage, but no points deducted.

**Part ex1-part3** (ex1-part3.code): 3/3 points

_Feedback:_ Good job: you used UMAP, split train/test, fit UMAP on train and transformed test, trained KNN, and computed accuracy. Using supervised UMAP (y in fit_transform) is acceptable. This meets the task’s goal of calculating KNN accuracy on UMAP features.

---

### Exercise 4 (17/20 = 85.0%)

**Part ex2-part1** (ex2-part1.code): 5/7 points

_Feedback:_ Good attempt: you apply PCA with multiple n_components, fit KNN, and provide a 2D scatter plot. However, you evaluate accuracy on the training data only; use train/test split with PCA fit on train and transform test. Optionally try PCA(.9) for variance-based components.

**Part ex2-part2** (ex2-part2.code): 7/7 points

_Feedback:_ Well done: you applied UMAP with multiple settings (2D/3D), used supervised fit, trained KNN on the embedding, reported accuracy, and plotted 2D cases. This correctly mirrors your prior PCA workflow. Note: in-sample accuracy can be optimistic, but no deduction here.

**Part ex2-part3** (ex2-part3.answer): 5/6 points

_Feedback:_ Good comparison: you note PCA’s drop in low dims and UMAP’s superior preservation of neighborhoods with strong accuracy. You explored parameters (2D/3D, neighbors), but you didn’t mention the effect of lower n_neighbors in 2D. Note that to earn full credit.

---

### Exercise 1 (20/20 = 100.0%)

**Part pipeline-part1** (pipeline-part1.code): 4/4 points

_Feedback:_ Correct and complete. You apply PCA to 2 components and produce a clear 2D scatter colored by digit labels with a colorbar and axis labels. Transforming the test set isn’t required for this step but doesn’t hurt. Nice work.

**Part pipeline-part2** (pipeline-part2.code): 4/4 points

_Feedback:_ Excellent scree plot. You fit a 40-component PCA on the training set and plotted percent variance explained for components 1–40 with appropriate labels. Correct and aligned with the task. Nice work.

**Part pipeline-part3** (pipeline-part3.code): 4/4 points

_Feedback:_ Correct approach. You fit PCA on the full feature set, computed cumulative explained variance, and selected the minimal number of components meeting 95% using searchsorted (+1). This meets the task and works independently of earlier PCA choices. Nice job.

**Part pipeline-part4** (pipeline-part4.code): 4/4 points

_Feedback:_ Correct: you refit PCA with the Step 4 dimensionality, projected and inverse-transformed the same digit, and plotted it. This satisfies the task. Minor suggestion: use the computed n_components_95 variable from Step 4 instead of hard-coding 148.

**Part pipeline-part5** (pipeline-part5.code): 4/4 points

_Feedback:_ Well done. You computed the number of components to preserve 80% variance, trained/evaluated KNN on raw and PCA-transformed data, and reported accuracies. You fit PCA on train and transformed test correctly. Minor nit: unused import (time) and random_state not needed.

---

## Additional Information

This feedback was automatically generated by the autograder using LLM-based evaluation.

**Generated:** 2025-11-11 17:33:47 UTC

If you have questions about your grade, please reach out to the instructor.

---

*Powered by [Grade-Lite](https://github.com/your-repo/grade-lite) Autograder*